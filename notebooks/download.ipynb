{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Test Download Performance with Streaming\n",
        "\n",
        "Questo notebook testa le funzionalitÃ  core del metodo `export` con streaming per migliorare le performance del download di dataset dall'API OpenData.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import asyncio\n",
        "import time\n",
        "import httpx\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Add backend to path\n",
        "sys.path.insert(0, str(Path.cwd().parent))\n",
        "\n",
        "from backend.opendata_api.client import BolognaOpenData\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Versione Non-Streaming con HTTP/2\n",
        "\n",
        "Test della versione che usa `r.content` con HTTP/2 abilitato.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing HTTP/2 download of: rilevazione-flusso-veicoli-tramite-spire-anno-2025\n",
            "âœ… HTTP/2 download completed in 99.36s\n",
            "ğŸ“¦ Size: 13.94 MB\n",
            "âš¡ Speed: 0.14 MB/s\n"
          ]
        }
      ],
      "source": [
        "async def export_http2(dataset_id: str, fmt: str = \"parquet\") -> bytes:\n",
        "    \"\"\"Versione con HTTP/2: carica tutto in memoria con r.content\"\"\"\n",
        "    # Create client with HTTP/2 enabled\n",
        "    client_http2 = httpx.AsyncClient(\n",
        "        base_url=\"https://opendata.comune.bologna.it/api/explore/v2.1\",\n",
        "        timeout=httpx.Timeout(connect=10.0, read=600.0, write=10.0, pool=5.0),\n",
        "        limits=httpx.Limits(max_keepalive_connections=10, max_connections=20, keepalive_expiry=60.0),\n",
        "        http2=True  # Enable HTTP/2\n",
        "    )\n",
        "    try:\n",
        "        r = await client_http2.get(\n",
        "            f\"/catalog/datasets/{dataset_id}/exports/{fmt}\"\n",
        "        )\n",
        "        r.raise_for_status()\n",
        "        return r.content\n",
        "    finally:\n",
        "        await client_http2.aclose()\n",
        "\n",
        "# Test\n",
        "dataset_id = \"rilevazione-flusso-veicoli-tramite-spire-anno-2025\"\n",
        "\n",
        "print(f\"Testing HTTP/2 download of: {dataset_id}\")\n",
        "start = time.time()\n",
        "data_http2 = await export_http2(dataset_id, \"parquet\")\n",
        "elapsed_http2 = time.time() - start\n",
        "size_mb = len(data_http2) / (1024 * 1024)\n",
        "\n",
        "print(f\"âœ… HTTP/2 download completed in {elapsed_http2:.2f}s\")\n",
        "print(f\"ğŸ“¦ Size: {size_mb:.2f} MB\")\n",
        "print(f\"âš¡ Speed: {size_mb/elapsed_http2:.2f} MB/s\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Versione con Streaming + Bytearray\n",
        "\n",
        "Versione ottimizzata che usa streaming con bytearray e chunk size di 2MB (implementata nel client).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Testing streaming + bytearray download of: rilevazione-flusso-veicoli-tramite-spire-anno-2025\n",
            "\n",
            "âœ… Streaming + bytearray download completed in 99.40s\n",
            "ğŸ“¦ Size: 13.94 MB\n",
            "âš¡ Speed: 0.14 MB/s\n"
          ]
        }
      ],
      "source": [
        "async def export_streaming_bytearray(\n",
        "    client: BolognaOpenData, \n",
        "    dataset_id: str, \n",
        "    fmt: str = \"parquet\"\n",
        ") -> bytes:\n",
        "    \"\"\"Versione esplicita con streaming + bytearray: mostra chiaramente lo streaming\"\"\"\n",
        "    await client._ensure_client_ready()\n",
        "    try:\n",
        "        # Use streaming with explicit async context manager\n",
        "        async with client._client.stream(\n",
        "            \"GET\",\n",
        "            f\"/catalog/datasets/{dataset_id}/exports/{fmt}\"\n",
        "        ) as response:\n",
        "            response.raise_for_status()\n",
        "            \n",
        "            # Use bytearray for efficient accumulation (faster than list + join)\n",
        "            buffer = bytearray()\n",
        "            async for chunk in response.aiter_bytes(chunk_size=512):  # 2MB chunks\n",
        "                buffer.extend(chunk)\n",
        "            \n",
        "            return bytes(buffer)\n",
        "            \n",
        "    except (RuntimeError, ConnectionError, httpx.ReadError) as e:\n",
        "        if \"closed\" in str(e).lower() or isinstance(e, (ConnectionError, httpx.ReadError)):\n",
        "            # Retry once on connection errors\n",
        "            await client._ensure_client_ready()\n",
        "            async with client._client.stream(\n",
        "                \"GET\",\n",
        "                f\"/catalog/datasets/{dataset_id}/exports/{fmt}\"\n",
        "            ) as response:\n",
        "                response.raise_for_status()\n",
        "                buffer = bytearray()\n",
        "                async for chunk in response.aiter_bytes(chunk_size=2 * 1024 * 1024):\n",
        "                    buffer.extend(chunk)\n",
        "                return bytes(buffer)\n",
        "        raise\n",
        "\n",
        "# Test\n",
        "client = BolognaOpenData()\n",
        "\n",
        "print(f\"Testing streaming + bytearray download of: {dataset_id}\")\n",
        "start = time.time()\n",
        "data_streaming = await export_streaming_bytearray(client, dataset_id, \"parquet\")\n",
        "elapsed_streaming = time.time() - start\n",
        "size_mb_streaming = len(data_streaming) / (1024 * 1024)\n",
        "\n",
        "print(f\"\\nâœ… Streaming + bytearray download completed in {elapsed_streaming:.2f}s\")\n",
        "print(f\"ğŸ“¦ Size: {size_mb_streaming:.2f} MB\")\n",
        "print(f\"âš¡ Speed: {size_mb_streaming/elapsed_streaming:.2f} MB/s\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âœ… Data integrity check passed: both methods return identical data\n",
            "\n",
            "ğŸ“Š Performance Comparison:\n",
            "Method                         Time (s)     Speed (MB/s)    vs HTTP/2      \n",
            "------------------------------------------------------------------------\n",
            "HTTP/2 (r.content)             99.36        0.14            baseline       \n",
            "Streaming + bytearray          99.40        0.14            -0.0%          \n",
            "\n",
            "âš ï¸  Streaming + bytearray is 0.0% slower than HTTP/2\n",
            "\n",
            "âœ… Client closed\n"
          ]
        }
      ],
      "source": [
        "# Verifica che i dati siano identici\n",
        "assert data_http2 == data_streaming, \"âŒ Data mismatch!\"\n",
        "print(\"âœ… Data integrity check passed: both methods return identical data\\n\")\n",
        "\n",
        "# Confronto\n",
        "print(\"ğŸ“Š Performance Comparison:\")\n",
        "print(f\"{'Method':<30} {'Time (s)':<12} {'Speed (MB/s)':<15} {'vs HTTP/2':<15}\")\n",
        "print(\"-\" * 72)\n",
        "print(f\"{'HTTP/2 (r.content)':<30} {elapsed_http2:<12.2f} {size_mb/elapsed_http2:<15.2f} {'baseline':<15}\")\n",
        "print(f\"{'Streaming + bytearray':<30} {elapsed_streaming:<12.2f} {size_mb_streaming/elapsed_streaming:<15.2f} {f'{((elapsed_http2 - elapsed_streaming) / elapsed_http2)*100:+.1f}%':<15}\")\n",
        "\n",
        "if elapsed_streaming < elapsed_http2:\n",
        "    improvement = ((elapsed_http2 - elapsed_streaming) / elapsed_http2) * 100\n",
        "    print(f\"\\nğŸš€ Streaming + bytearray is {improvement:.1f}% faster than HTTP/2!\")\n",
        "else:\n",
        "    slowdown = ((elapsed_streaming - elapsed_http2) / elapsed_http2) * 100\n",
        "    print(f\"\\nâš ï¸  Streaming + bytearray is {slowdown:.1f}% slower than HTTP/2\")\n",
        "\n",
        "# Cleanup\n",
        "await client.close()\n",
        "print(\"\\nâœ… Client closed\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“¥ Step 1: Downloading rilevazione-flusso-veicoli-tramite-spire-anno-2025 with HTTP/2...\n",
            "   âœ… Download completed in 104.70s\n",
            "   ğŸ“¦ Size: 13.94 MB\n",
            "   âš¡ Download speed: 0.13 MB/s\n",
            "\n",
            "ğŸ“¤ Step 2: Uploading to S3 (input/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2025.parquet)...\n",
            "   âœ… Upload completed in 1.73s\n",
            "   âš¡ Upload speed: 8.05 MB/s\n",
            "   ğŸ”— S3 location: s3://lg-urban-prod/input/datasets/rilevazione-flusso-veicoli-tramite-spire-anno-2025.parquet\n",
            "\n",
            "ğŸ“Š Summary:\n",
            "   Operation            Time (s)        Speed (MB/s)   \n",
            "   -------------------- --------------- ---------------\n",
            "   Download (HTTP/2)    104.70          0.13           \n",
            "   Upload (S3)          1.73            8.05           \n",
            "   TOTAL                106.43                         \n",
            "\n",
            "   ğŸ“ˆ Upload overhead: 1.73s (1.6% of total time)\n",
            "   âœ… Verified: file exists in S3\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import boto3\n",
        "from botocore.client import Config\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "# Check S3 configuration\n",
        "load_dotenv()\n",
        "s3_bucket = os.getenv(\"S3_BUCKET\")\n",
        "if not s3_bucket:\n",
        "    print(\"âš ï¸  S3_BUCKET not set. Skipping S3 upload test.\")\n",
        "    print(\"   Set S3_BUCKET, AWS_ACCESS_KEY_ID, AWS_SECRET_ACCESS_KEY to test upload.\")\n",
        "else:\n",
        "    # 1. Download con HTTP/2\n",
        "    print(f\"ğŸ“¥ Step 1: Downloading {dataset_id} with HTTP/2...\")\n",
        "    download_start = time.time()\n",
        "    data_downloaded = await export_http2(dataset_id, \"parquet\")\n",
        "    download_time = time.time() - download_start\n",
        "    size_mb = len(data_downloaded) / (1024 * 1024)\n",
        "    \n",
        "    print(f\"   âœ… Download completed in {download_time:.2f}s\")\n",
        "    print(f\"   ğŸ“¦ Size: {size_mb:.2f} MB\")\n",
        "    print(f\"   âš¡ Download speed: {size_mb/download_time:.2f} MB/s\\n\")\n",
        "    \n",
        "    # 2. Upload su S3\n",
        "    print(f\"ğŸ“¤ Step 2: Uploading to S3 (input/datasets/{dataset_id}.parquet)...\")\n",
        "    upload_start = time.time()\n",
        "    \n",
        "    # Setup S3 client\n",
        "    region = os.getenv(\"AWS_REGION\", \"eu-central-1\")\n",
        "    s3_client = boto3.client(\n",
        "        \"s3\",\n",
        "        region_name=region,\n",
        "        config=Config(signature_version='s3v4')\n",
        "    )\n",
        "    \n",
        "    # S3 key: input/datasets/{dataset_id}.parquet (coerente con load_dataset_tool)\n",
        "    s3_key = f\"input/datasets/{dataset_id}.parquet\"\n",
        "    \n",
        "    # Upload to S3\n",
        "    s3_client.put_object(\n",
        "        Bucket=s3_bucket,\n",
        "        Key=s3_key,\n",
        "        Body=data_downloaded,\n",
        "        ContentType=\"application/parquet\"\n",
        "    )\n",
        "    \n",
        "    upload_time = time.time() - upload_start\n",
        "    \n",
        "    print(f\"   âœ… Upload completed in {upload_time:.2f}s\")\n",
        "    print(f\"   âš¡ Upload speed: {size_mb/upload_time:.2f} MB/s\")\n",
        "    print(f\"   ğŸ”— S3 location: s3://{s3_bucket}/{s3_key}\\n\")\n",
        "    \n",
        "    # 3. Summary\n",
        "    total_time = download_time + upload_time\n",
        "    print(\"ğŸ“Š Summary:\")\n",
        "    print(f\"   {'Operation':<20} {'Time (s)':<15} {'Speed (MB/s)':<15}\")\n",
        "    print(f\"   {'-'*20} {'-'*15} {'-'*15}\")\n",
        "    print(f\"   {'Download (HTTP/2)':<20} {download_time:<15.2f} {size_mb/download_time:<15.2f}\")\n",
        "    print(f\"   {'Upload (S3)':<20} {upload_time:<15.2f} {size_mb/upload_time:<15.2f}\")\n",
        "    print(f\"   {'TOTAL':<20} {total_time:<15.2f} {'':<15}\")\n",
        "    print(f\"\\n   ğŸ“ˆ Upload overhead: {upload_time:.2f}s ({upload_time/total_time*100:.1f}% of total time)\")\n",
        "    \n",
        "    # Verify upload\n",
        "    try:\n",
        "        s3_client.head_object(Bucket=s3_bucket, Key=s3_key)\n",
        "        print(f\"   âœ… Verified: file exists in S3\")\n",
        "    except Exception as e:\n",
        "        print(f\"   âš ï¸  Warning: could not verify S3 upload: {e}\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "langgraph-py3.11",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
