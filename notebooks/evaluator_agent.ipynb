{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f34ba9e",
   "metadata": {},
   "source": [
    "## Evaluator Agent\n",
    "\n",
    "Here we want to implement an agent that validates the analysis of the data analyst based on the sources and the chat history. \n",
    "\n",
    "A simple reference to do it quickly: https://docs.langchain.com/oss/python/langgraph/workflows-agents#evaluator-optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5ed32a8",
   "metadata": {},
   "source": [
    "## Brainstorm\n",
    "\n",
    "This evaluator should look at the sources, which will contain metadata like urls, description. \n",
    "\n",
    "Maybe we give online search as well to validate those data? Nah too broad: we give it dataset look up in the opendata to validate the sources and see if the model hallucinated! For example does that dataset_id exist? do the descriptions match? \n",
    "\n",
    "Ok; does the evaluator also look at the python code written? Well, he will automatically see it in chat history (if it wasn't summarized). Does he also see the reasoning? do those reasoning messages get included in chat history? They should, but check. \n",
    "I don't think it makes much sense to make it look at the code - if it errored the analyst sees the error, if it worked it worked. \n",
    "\n",
    "Maybe look at the tools used? If the answers match the tool use or if they were hallucinated? \n",
    "\n",
    "Also maybe it looks at the first message from the user and checks it√¨f the request was actually answered? \n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
